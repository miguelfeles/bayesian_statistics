{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood in Statistics\n",
    "\n",
    "## What is Likelihood?\n",
    "\n",
    "Likelihood refers to a concept in statistics that measures how probable a particular set of observations is, given specific parameters of a statistical model. Unlike probability, which predicts future outcomes based on known parameters, likelihood works in the opposite direction: it evaluates the plausibility of model parameters given observed data.\n",
    "\n",
    "## Importance of Likelihood\n",
    "\n",
    "The concept of likelihood is central to many statistical methods, including maximum likelihood estimation (MLE), where it's used to estimate the parameters of a statistical model. By maximizing the likelihood function, statisticians can find the parameter values that are most likely to have resulted in the observed data, thereby fitting the model to the data effectively.\n",
    "\n",
    "## Likelihood vs. Probability\n",
    "\n",
    "- **Probability** is used to predict the likelihood of future data occurrences given known parameters.\n",
    "- **Likelihood**, on the other hand, assesses the plausibility of parameter values given the data already observed.\n",
    "\n",
    "## Example: Tossing a Coin\n",
    "\n",
    "Consider the scenario of tossing a fair coin three times, resulting in two heads and one tail. The likelihood function in this case would quantify how likely it is to observe this specific outcome for different biases of the coin (parameter values), such as a fair coin (50% heads) or a biased coin (e.g., 60% heads).\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- Likelihood provides a way to assess different parameter values for a statistical model based on observed data.\n",
    "- It is a fundamental concept in statistical inference and is crucial for parameter estimation and model selection.\n",
    "- Understanding likelihood allows statisticians and data scientists to make informed decisions about the models they use to describe real-world phenomena.\n",
    "\n",
    "Remember, while likelihood and probability are related concepts, they are used differently within the context of statistical analysis. Likelihood focuses on evaluating parameter values based on observed data, making it an essential tool for statistical modeling and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Likelihood with a Coin Toss Example\n",
    "\n",
    "In statistics, the concept of likelihood helps us understand how plausible different parameter values are, given observed data. Let's explore this through a simple example involving tossing a coin.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You toss a coin three times, observing the sequence: Heads, Heads, Tails (HHT).\n",
    "\n",
    "## Parameter (\\(\\theta\\))\n",
    "\n",
    "- \\(\\theta\\) represents the probability of the coin landing on heads.\n",
    "- For a fair coin, \\(\\theta = 0.5\\). However, the coin could be biased, meaning \\(\\theta\\) could vary between 0 and 1.\n",
    "\n",
    "## Changing Parameters\n",
    "\n",
    "To understand how the likelihood changes with \\(\\theta\\), consider:\n",
    "\n",
    "- **Fair Coin (\\(\\theta = 0.5\\))**: The likelihood of observing HHT is \\(0.5^2 \\times 0.5 = 0.125\\).\n",
    "- **Biased Coin (\\(\\theta = 0.6\\))**: The likelihood increases to \\(0.6^2 \\times 0.4 = 0.144\\).\n",
    "- **More Biased Coin (\\(\\theta = 0.7\\))**: The likelihood is \\(0.7^2 \\times 0.3 = 0.147\\).\n",
    "\n",
    "## Observations\n",
    "\n",
    "- The likelihood of observing HHT changes as we adjust \\(\\theta\\), reflecting different biases of the coin.\n",
    "- By comparing likelihoods across different \\(\\theta\\) values, we identify which bias (\\(\\theta\\) value) makes the observed outcome most plausible.\n",
    "- This example illustrates that a slight bias towards heads (\\(\\theta > 0.5\\)) may be more consistent with observing HHT.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The essence of the likelihood concept in statistical inference is to evaluate how plausible different parameter values (\\(\\theta\\)) are, given the data observed. In our coin toss example, varying \\(\\theta\\) allows us to hypothesize about the coin's bias and determine which hypothesis best explains the observed data (HHT). The \\(\\theta\\) that maximizes the likelihood function is considered the most likely estimate of the coin's true bias based on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heads on a particular throw is confined to one of six discrete values: θ ∈ {0, 0.2, 0.4, 0.6, 0.8, 1.0}. Using this information we compute the various probabilities of each possible outcome, which are displayed in Table 4.1.\n",
    "In tabular form, we can see the effect of varying the data (moving along each row) and contrast it with the effect of varying θ (moving down each column).\n",
    "If we hold the parameter fixed – regardless of the value of θ – and vary the data by moving along each row, the values sum to 1, meaning that this is a valid probability distribution. By contrast, when we hold the number of heads fixed and vary the parameter θ, by moving down each column the values do not sum to 1. When θ varies we do not have a valid probability distribution, meriting the use of the term likelihood.\n",
    "In Bayesian inference, we always vary the parameter and hold the data fixed (we only obtain one sample). Thus, from a Bayesian perspective, we use the term likelihood to remind us that p(data|θ) is not a probability distribution.\n",
    "Table 4.1  The probabilities/likelihoods for two coin flips, where the probability of heads (θ) is confined to the discrete values: {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}. X is the number of heads we obtain in two throws of the coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Probability of coin landing heads up, θ | Number of heads, X |\n",
    "|----------------------------------------|---------------------|\n",
    "|                                        | 0     | 1    | 2    |\n",
    "|----------------------------------------|-------|------|------|\n",
    "| 0.0                                    | 1.00  | 0.00 | 0.00 |\n",
    "| 0.2                                    | 0.64  | 0.32 | 0.04 |\n",
    "| 0.4                                    | 0.36  | 0.48 | 0.16 |\n",
    "| 0.6                                    | 0.16  | 0.48 | 0.36 |\n",
    "| 0.8                                    | 0.04  | 0.32 | 0.64 |\n",
    "| 1.0                                    | 0.00  | 0.00 | 1.00 |\n",
    "|----------------------------------------|-------|------|------|\n",
    "| Total                                  | 2.20  | 1.60 | 2.20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Functions vs. Probability Distributions\n",
    "\n",
    "## Probability Distributions\n",
    "- **Definition**: Mathematical functions that give the probabilities of different outcomes.\n",
    "- **Requirement**: Probabilities must sum to 1 across all possible outcomes.\n",
    "\n",
    "## Likelihood Functions\n",
    "- **Definition**: Functions of parameters given observed data, indicating how well the parameters explain the data.\n",
    "- **Key Property**: The values of a likelihood function do not sum to 1 over parameter values.\n",
    "\n",
    "## Why Don't Likelihoods Sum to 1?\n",
    "- **Different Purpose**: Likelihoods are not probabilities but measures of plausibility for parameters given data.\n",
    "- **Parameter Estimation**: Likelihoods are used to estimate parameters, not to describe the complete range of a random variable's outcomes.\n",
    "- **Model Fitting**: They help in fitting statistical models to observed data, unlike probability distributions which predict data given known parameters.\n",
    "\n",
    "## Bayesian Inference and Likelihood\n",
    "- **Role in Bayesian Inference**: Likelihoods are combined with prior distributions to form posterior distributions.\n",
    "- **Normalization**: The posterior distribution, which is a probability distribution, will sum to 1 by the process of normalization.\n",
    "\n",
    "## Conclusion\n",
    "- **No Problem**: It is not a problem that likelihoods do not sum to 1; it is by design and serves a different purpose from probability distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exchangeability and Random Sampling\n",
    "\n",
    "## Exchangeability\n",
    "\n",
    "- **Definition**: \n",
    "  - A sequence of random variables is *exchangeable* if their joint probability distribution is invariant to permutations. The order of data does not affect their joint probability.\n",
    "\n",
    "- **Implications**:\n",
    "  - Exchangeability suggests that each data point is equally informative about the underlying distribution and there is no inherent ordering to the data.\n",
    "\n",
    "- **In Bayesian Statistics**:\n",
    "  - Exchangeability allows for modeling without specifying the data's sequence, often implying a shared underlying parameter influencing all data points.\n",
    "\n",
    "## Random Sampling\n",
    "\n",
    "- **Definition**: \n",
    "  - *Random sampling* is a technique where each subset of data from a population has an equal chance of selection, ensuring each sample is unbiased and representative.\n",
    "\n",
    "- **Requirement**:\n",
    "  - Assumes each data point is independent and identically distributed (i.i.d.), which is essential for the representativeness of the sample.\n",
    "\n",
    "## Connection Between Exchangeability and Random Sampling\n",
    "\n",
    "- **From Random to Exchangeable**: \n",
    "  - A random sample is by definition exchangeable, as the randomness ensures the irrelevance of order.\n",
    "\n",
    "- **From Exchangeable to Random**:\n",
    "  - If a sample is exchangeable, it can often be treated as a random sample because the order does not convey additional information.\n",
    "\n",
    "## Practical Implication\n",
    "\n",
    "- **Exchangeability as a Safety Net**:\n",
    "  - In real-world scenarios where true random sampling is challenging, assuming exchangeability provides a basis for robust statistical inference.\n",
    "\n",
    "In essence, exchangeability allows us to treat a sample as if it were randomly drawn, even when perfect randomness in the sampling process is unattainable. This assumption is fundamental to many statistical techniques, particularly in Bayesian inference, where it justifies the use of likelihoods in updating beliefs based on observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Overview\n",
    "- **Purpose**: MLE is used to find the parameter values that make the observed data most probable under a specified statistical model.\n",
    "\n",
    "## The Likelihood Function\n",
    "- **Definition**: \n",
    "  - A likelihood function `L(θ | data)` is a function of the parameters `θ` that measures the probability of the observed data under those parameters.\n",
    "  - Unlike a probability distribution, it is not normalized; it's a relative measure.\n",
    "\n",
    "## Estimating Parameters\n",
    "- **Process**:\n",
    "  - Choose a model with parameters `θ` that could have generated the data.\n",
    "  - Define the likelihood function for this model given the observed data.\n",
    "  - Find the parameter values `θ` that maximize this function.\n",
    "\n",
    "## Maximization\n",
    "- **Techniques**:\n",
    "  - Often involves taking the derivative of the likelihood function with respect to `θ`, setting it to zero, and solving for `θ`.\n",
    "  - In practice, it's common to work with the natural logarithm of the likelihood function, known as the log-likelihood, since it simplifies the calculus and the maximization problem without affecting the parameter estimates.\n",
    "\n",
    "## Properties of MLE\n",
    "- **Consistency**: \n",
    "  - As the sample size increases, the MLE tends to converge to the true parameter value.\n",
    "- **Efficiency**: \n",
    "  - Among all unbiased estimators, MLE tends to have the smallest variance.\n",
    "- **Asymptotic Normality**:\n",
    "  - As the sample size grows, the distribution of MLE tends to approach a normal distribution.\n",
    "\n",
    "## Practical Considerations\n",
    "- **Complex Models**:\n",
    "  - For models that result in complex likelihood functions, numerical methods may be required to find the MLE.\n",
    "- **Limitations**:\n",
    "  - MLE assumes the model is correct and can be biased for small sample sizes.\n",
    "\n",
    "MLE is a foundational tool in statistics for parameter estimation, widely used for its strong theoretical properties and practical effectiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of heads (p): 0.6000000000000003\n",
      "Confidence Interval: [[0.29636369 0.90363631]]\n",
      "Log-Likelihood: -6.730116670092565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:2694: UserWarning: df_model + k_constant differs from nparams\n",
      "  warnings.warn(\"df_model + k_constant differs from nparams\")\n",
      "/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:2696: UserWarning: df_resid differs from nobs - nparams\n",
      "  warnings.warn(\"df_resid differs from nobs - nparams\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "# Sample data: 1 represents heads, 0 represents tails\n",
    "data = np.array([1, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "# Define a class for the binomial model\n",
    "class BinomialModel(GenericLikelihoodModel):\n",
    "    def loglike(self, params):\n",
    "        p = params[0]\n",
    "        # Calculate the log-likelihood for binomial distribution\n",
    "        log_likelihood = np.sum(self.endog * np.log(p) + (1 - self.endog) * np.log(1 - p))\n",
    "        return log_likelihood\n",
    "\n",
    "# Instantiate the model with data\n",
    "model = BinomialModel(data)\n",
    "\n",
    "# Fit the model by maximizing the log-likelihood\n",
    "results = model.fit(start_params=np.array([0.5]), method='nm', disp=0)\n",
    "\n",
    "# Estimated parameter (probability of heads)\n",
    "p_hat = results.params[0]\n",
    "\n",
    "print(f\"Estimated probability of heads (p): {p_hat}\")\n",
    "print(f\"Confidence Interval: {results.conf_int()}\")\n",
    "\n",
    "# Evaluate the goodness of fit\n",
    "log_likelihood = model.loglike(results.params)\n",
    "print(f\"Log-Likelihood: {log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Interpretation of Binomial Distribution Parameter Estimation\n",
    "\n",
    "Given a dataset from coin flips, where `1` represents heads and `0` represents tails, we use Maximum Likelihood Estimation (MLE) to estimate the probability of flipping heads (`p`).\n",
    "\n",
    "## Results from Python MLE:\n",
    "- The `statsmodels` library allows us to fit our model to the data, maximizing the log-likelihood to find our parameter estimate.\n",
    "\n",
    "### Estimated Probability of Heads (`p_hat`):\n",
    "- The value of `p_hat` is the MLE for the probability of flipping heads.\n",
    "- This is the value that, given our model, makes the observed data most likely.\n",
    "\n",
    "### Confidence Interval:\n",
    "- The confidence interval provides a range of plausible values for `p` based on the observed data.\n",
    "- A typical confidence interval used is 95%, which informs us that, if we were to repeat our experiment many times, 95% of the confidence intervals calculated from those experiments would contain the true `p`.\n",
    "\n",
    "### Log-Likelihood:\n",
    "- The log-likelihood is a measure of the probability of observing the given data under the estimated model parameters.\n",
    "- A higher log-likelihood means a better fit of the model to the data.\n",
    "\n",
    "## Interpretation:\n",
    "- The MLE gives us a point estimate for the parameter `p`. It tells us the most likely proportion of heads in a coin flip based on our data.\n",
    "- The confidence interval around `p_hat` gives us an idea of the precision of our estimate.\n",
    "- We assume the coin flips are independent events, and thus the likelihood of each sequence of coin flips is simply the product of the likelihoods of individual flips.\n",
    "\n",
    "## Considerations:\n",
    "- This example assumes a binomial model is appropriate for the data.\n",
    "- It is essential to check model assumptions in practice, which can include the independence of events and the appropriateness of the binomial distribution for the data.\n",
    "\n",
    "This simplified example illustrates how MLE is used to estimate parameters that explain our data under a given model. The `statsmodels` library facilitates this process by providing tools to maximize the likelihood function and compute confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

